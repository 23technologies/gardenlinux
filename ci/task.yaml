apiVersion: tekton.dev/v1beta1
kind: Task
metadata:
  name: build-gardenlinux-task
  namespace: gardenlinux-tkn
spec:
  params:
    - name: 'suite'
      type: 'string'
      default: 'bullseye'
      description: 'Debian release (buster, bullseye, ..)'
    - name: 'architecture'
      type: 'string'
      description: 'the build architecture (currently, only amd64 is supported)'
      default: 'amd64'
    - name: 'platform'
      type: 'string'
      description: 'the target platform (aws, gcp, metal, kvm, ..)'
    - name: 'modifiers'
      type: 'string'
      description: 'the build modifiers'
    - name: 'uploadprefix'
      type: 'string'
      description: 'upload directory prefix (aws, gcp, ..)'
    - name: 'fnameprefix'
      type: 'string'
      description: 'upload filename prefix (workaround until garden-build.sh supports this'
    - name: 'giturl'
      type: 'string'
      default: 'ssh://git@github.com/gardenlinux/gardenlinux'
      description: 'git repository url'
    - name: 'committish'
      type: 'string'
      description: 'the committish to build'
      default: 'master'
    - name: 'repodir'
      description: 'path to gardenlinux repository worktree'
      default: '/workspace/gardenlinux_git'
    - name: 'cicd_cfg_name'
      description: 'the cicd cfg to use (see cicd.yaml)'
      default: 'default'
    - name: 'outfile'
      description: 'build result file (parameter is used to pass between steps)'
      type: 'string'
      default: '/workspace/gardenlinux.out'
  steps:
    - name: 'retrieve-repository'
      image: 'eu.gcr.io/gardener-project/cc/job-image:1.612.0'
      script: |
        #!/usr/bin/env python3
        import os
        import urllib.parse
        import shutil

        import ccc.github
        import gitutil

        repo_dir = os.path.abspath('$(params.repodir)')
        os.mkdir(repo_dir)
        repo_url = urllib.parse.urlparse('$(params.giturl)')
        github_cfg = ccc.github.github_cfg_for_hostname(
          repo_url.hostname,
        )
        git_helper = gitutil.GitHelper.clone_into(
          target_directory=repo_dir,
          github_cfg=github_cfg,
          github_repo_path=repo_url.path,
        )
        git_helper.repo.git.checkout('$(params.committish)')

        print(f'cloned into {repo_dir} - checked out: $(params.committish)')
        print(git_helper.repo.git.show())

    - name: 'build-image'
      resources:
        requests:
          memory: 1Gi
        limits:
          memory: 1.5Gi
      securityContext:
        privileged: true
        allowPrivilegeEscalation: true
        capabilities:
          add:
            - 'SYS_ADMIN'
      image: eu.gcr.io/gardener-project/gardenlinux/imagebuild_image:0.10.0-SAP
      script: |
        #!/usr/bin/env sh
        set -u
        set -x
        export TZ=UTC
        export LC_ALL=C
        export suite="$(params.suite)"
        export timestamp="$(date -d 'today' '+%Y%m%d')"
        # XXX hardcode `server,cloud` until dependencies handling is implemented
        export features="$(params.platform),$(params.modifiers),server,cloud"
        #export debug=''
        export qemu=''
        export eol=''
        export ports=''
        echo "features: ${features}"
        export OUT_DIR="/build"
        ls -la /build
        export OUT_FILE="$(params.outfile)"

        pwd
        echo "running build.."
        #/opt/debuerreotype/bin/build.sh
        $(params.repodir)/bin/garden-build.sh \
          --arch $(params.architecture) \
          --prefix "" \
          --suite $(params.suite) \
          --timestamp ${timestamp} \
          --features "${features}"
        ls -la "${OUT_FILE}"
        tar tf "${OUT_FILE}"
        if [ -f "${OUT_FILE}" ]; then
          echo "seems, like we might have succeeded?"
        else
          echo "no archive was created - see build log above for errors"
          exit 1
        fi
      volumeMounts:
      - mountPath: '/dev'
        name: 'dev'
      - mountPath: '/build'
        name: 'build'

    - name: 'upload-results'
      image: 'eu.gcr.io/gardener-project/cc/job-image:1.612.0'
      script: |
        #!/usr/bin/env python3
        # requires github.com/gardener/cc-utils
        import dataclasses
        import datetime
        import os
        import sys
        # XXX add code from `ci/` to PYTHONPATH
        sys.path.insert(1, os.path.abspath(os.path.join('$(params.repodir)', 'ci')))
        import io

        import hashlib
        import yaml

        import glci.model
        import glci.util

        build_result_fname = '$(params.outfile)'
        if not os.path.isfile(build_result_fname):
          print('ERROR: no build result - see previous step for errs')
          import sys
          sys.exit(1)

        os.environ['SECRETS_SERVER_ENDPOINT'] = 'http://secrets-server.concourse.svc.cluster.local'
        os.environ['SECRETS_SERVER_CONCOURSE_CFG_NAME'] = 'concourse-secrets/concourse_cfg'
        import ccc.aws
        import tarfile
        cicd_cfg = glci.util.cicd_cfg(cfg_name='$(params.cicd_cfg_name)')
        aws_cfg_name = cicd_cfg.build.aws_cfg_name
        s3_bucket_name = cicd_cfg.build.s3_bucket_name
        session = ccc.aws.session(aws_cfg_name)
        s3_client = session.client('s3')
        print(f'uploading to s3 {aws_cfg_name=} {s3_bucket_name=}')
        version_str = '$(params.committish)'[:6]
        prefix = '$(params.uploadprefix)'
        fname_prefix = '$(params.fnameprefix)'

        def upload_fname(suffix: str):
          return f'{fname_prefix}-{version_str}-{suffix}'

        def upload_files():
          with tarfile.open(build_result_fname) as tf:
            for tarinfo in tf:
              if not tarinfo.isfile():
                continue
              tar_fname = tarinfo.name.lstrip('./')
              fname = upload_fname(suffix=tar_fname)
              target_name = os.path.normpath(os.path.join(prefix, fname))
              print(f'uploading: {target_name=}')
              fobj = tf.extractfile(tarinfo)

              # calculate filehash (use as fname)
              sha1 = hashlib.sha1()
              while (chunk := fobj.read(2048)):
                sha1.update(chunk)
              fobj.seek(0)
              sha1_digest = sha1.hexdigest()
              upload_key = os.path.join('objects', sha1_digest)

              # XXX todo: add content-type
              s3_client.upload_fileobj(
                Fileobj=fobj,
                Bucket=s3_bucket_name,
                Key=upload_key,
              )
              print(f'upload succeeded: {upload_key}')
              yield glci.model.ReleaseFile(
                rel_path=upload_key,
                name=fname,
                suffix=tar_fname,
              )

        uploaded_relpaths = tuple(upload_files())

        modifiers = [
          modifier_str for modifier_str
          in '$(params.modifiers)'.split(',')
        ]

        # add manifest (to be able to identify all relevant artifacts later)
        manifest = glci.model.ReleaseManifest(
          build_committish='$(params.committish)',
          build_timestamp=datetime.datetime.now().isoformat(),
          gardenlinux_epoch=glci.model.gardenlinux_epoch(),
          architecture=glci.model.Architecture('$(params.architecture)').value,
          platform='$(params.platform)',
          modifiers=modifiers,
          paths=uploaded_relpaths,
        )

        manifest_bytes = yaml.safe_dump(dataclasses.asdict(manifest)).encode('utf-8')
        manifest_fobj = io.BytesIO(initial_bytes=manifest_bytes)
        manifest_path = \
          f'{glci.model.ReleaseManifest.manifest_key_prefix}/snapshots/{fname_prefix}-{version_str}'

        s3_client.upload_fileobj(
          Fileobj=manifest_fobj,
          Bucket=s3_bucket_name,
          Key=manifest_path,
          ExtraArgs={
            'ContentType': 'text/yaml',
            'ContentEncoding': 'utf-8',
          },
        )
        print(f'uploaded manifest: {manifest_path}')
      volumeMounts:
      - mountPath: '/build'
        name: 'build'

  volumes:
    - name: dev
      hostPath:
        path: '/dev'
        type: 'Directory'
    - name: build
      emptyDir:
        medium: "Memory"
